# 3D Object Detection With A Pytorch PointPillars Network

**Table Of Contents**
- [Description](#description)
- [How does this sample work?](#how-does-this-sample-work)
    * [Processing the input graph](#processing-the-input-graph)
    * [Data preparation](#data-preparation)
    * [samplePointPillars plugins](#sample-pointpillars-plugins)
    * [Verifying the output](#verifying-the-output)
    * [TensorRT API layers and ops](#tensorrt-api-layers-and-ops)
- [Preparing sample data](#preparing-sample-data)
- [Running the sample](#running-the-sample)
    * [Sample `--help` options](#sample-help-options)
- [Additional resources](#additional-resources)
- [License](#license)
- [Changelog](#changelog)
- [Known issues](#known-issues)

## Description
This sample, samplePointPillars, serves as a demo on how to use a Pytorch based PointPillars model. It uses the `VoxelGeneratorPlugin`, `PillarScatterPlugin` and `DecodeBbox3DPlugin` TensorRT plugins to implement the VoxelGenerator layer, PillarScatter layer and DecodeBbox3D layer as custom layers since TensorRT has no native support for them.

## How does this sample work?

The ONNX PointPillars network performs the task of 3D object detection and localization in a single forward pass of the network with point cloud data. The PointPillars network was KITTI dataset to detect 3 classes of objects: `car`, `pedestrian`, and `cyclist`.

This sample makes use of TensorRT plugins to run the ONNX PointPillars network. To use these plugins, the ONNX graph needs to be preprocessed, and we use the onnx-graphsurgeon utility to do this.

The main components of this network are the VoxelGenerator, PillarFeatureNetwork, PillarScatter, Backbone, DetectionHead and Postprocessor.

**VoxelGenerator**
The VoxelGenerator generates voxels from the raw points from a point cloud frame(or a batch of frames). The voxels are essentially a spacial descretization of continuous points in 3D space. This is implemented as the `VoxelGeneratorPlugin`.

**PillarFeatureNetwork**
The PillarFeatureNetwork performs channel expansion and point reduction on voxels generated by VoxelGenerator. The output of PillarFeatureNetwork has only feature dimension and no individual points.

**PillarScatter**
The PillarScatter layer will convert the transformed voxels back into a dense format, so it can be easily processed by a ordinary 2D convolution based networks afterward. This is implemented as the `PillarScatterPlugin`.

**Backbone**
The 2D convolution based backbone network used to extract the features from the voxels.

**DetectionHead**
The detection head of the model, similar to the head in SSD model.

**PostProcessor**
Postprocessing of the output tensors from the detection head. The post-processor will finally produce the detected objects. The bounding box decoding part of this step is implemented as the `DecodeBbox3DPlugin`.


Specifically, this sample performs the following steps:
- [Processing the input graph](#processing-the-input-graph)
- [Preparing the data](#preparing-the-data)
- [samplePointPillars plugins](#sample-pointpillars-plugins)
- [Verifying the output](#verifying-the-output)


### Processing the input graph

The ONNX graph needs to be manupulated to insert the plugin nodes. The export from Pytorch model and node manupulation are done by the `export.py` script.

### Data preparation

The data input of PointPillars model is relatively simple. Only raw points are required to run this model. The raw points is read from binary point cloud files directly.

### samplePointPillars plugins

Details about how to create TensorRT plugins can be found in [Extending TensorRT With Custom Layers](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#extending).

The `export.py` script will do ONNX model export from Pytorch model and graph manipulation with onnx-graphsurgeon in one step. The script will insert plugin nodes in the ONNX model. The plugins used in this model are described below.

**VoxelGenerator plugin**
The `voxelGeneratorPlugin` performs the generation of voxels(pillars) from raw points in a point cloud frame. This operation essentially quantize the 3D points in spacial dimensions(x, y, z) with a certain granularity. The output of this plugin will be a group of pillars.

**PillarScatter plugin**
The `pillarScatterPlugin` performs scatter of voxels for PointPillars model. This operation is roughly a sparse to dense conversion and similar to ordinary scatter operation. The difference is `pillarScatterPlugin` will only scatter voxels that are marked as valid and do nothing for invalid voxels.

**DecodeBbox3D plugin**
The `decodeBbox3DPlugin` performs 3D bounding boxes decoding for PointPillars model.

### Verifying the output

After the builder is created (see [Building An Engine In C++](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#build_engine_c)) and the engine is serialized (see [Serializing A Model In C++](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#serial_model_c)), we can perform inference. Steps for de-serialization and running inference are outlined in [Performing Inference In C++](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#perform_inference_c). The outputs of the ONNX PointPillars network are human interpretable. The results are visualized by drawing the bounding boxes on the images.

### TensorRT API layers and ops

In this sample, the following layers are used. For more information about these layers, see the [TensorRT Developer Guide: Layers](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#layers) documentation.

[Activation layer](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#activation-layer)
The Activation layer implements element-wise activation functions. Specifically, this sample uses the Activation layer with the type `kRELU`.

[Convolution layer](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#convolution-layer)
The Convolution layer computes a 2D (channel, height, and width) convolution, with or without bias.

[FullyConnected layer](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#fullyconnected-layer)
The FullyConnected layer implements a matrix-vector product, with or without bias.

[Plugin layer](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#plugin-layer)
Plugin layers are user-defined and provide the ability to extend the functionalities of TensorRT. See [Extending TensorRT With Custom Layers](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#extending) for more details.

[Pooling layer](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#pooling-layer)
The Pooling layer implements pooling within a channel. Supported pooling types are `maximum`, `average` and `maximum-average blend`.

[Scale layer](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#scale-layer)
The Scale layer implements a per-tensor, per-channel, or per-element affine transformation and/or exponentiation by constant values.

## Preparing sample model and data

1. Download KITTI data. Download the 3 zip files:

    * [data_object_velodyne.zip](http://www.cvlibs.net/download.php?file=data_object_velodyne.zip)

    * [dat_object_image_2.zip](http://www.cvlibs.net/download.php?file=data_object_image_2.zip)

    * [data_object_calib.zip](http://www.cvlibs.net/download.php?file=data_object_calib.zip)

    Save them to `TensorRT/data/pointpillars`.

1. Pull and build OpenPCDet
    ```bash
    cd TensorRT
    nvidia-docker run -it --rm -v $(pwd):/workspace/TensorRT --network=host scrin/dev-spconv
    cd /workspace/TensorRT/data/pointpillars
    git clone https://github.com/open-mmlab/OpenPCDet.git
    cd OpenPCDet
    git checkout v0.5.0
    pip install -r requirements.txt
    python setup.py develop
    ```

2. Download and export Pytorch model to ONNX and do onnx-graphsurgeon.

    ```bash
    # From inside the container of step 1.
    cd tools
    cp -r ../../../../samples/samplePointPillars/export .
    pip install -r export/requirements.txt
    # Download Pytorch pretrained model
    python export/download_model.py ../../pointpillar_7728.pth
    # Export model to ONNX
    python export/export.py --ckpt ../../pointpillar_7728.pth -o ../../pointpillars.onnx
    # Preprocess data
    cd ../../
    unzip data_object_velodyne.zip
    unzip dat_object_image_2.zip
    unzip data_object_calib.zip
    mkdir fov_points
    python OpenPCDet/tools/export/fov_points.py -p training/velodyne -c training/calib -o fov_points -i training/image_2
    cp fov_points/000001.bin .
    # Exit the container
    exit
    ```

## Running the sample

1. Compile the sample by following build instructions in [TensorRT README](https://github.com/NVIDIA/TensorRT/).

2. Run the sample to perform object detection and localization.

   To run the sample:
    ```bash
    cd TensorRT
    mkdir -p build && cd build
    cmake CUDA_VERSION=<CUDA_VERSION> ..
    make -j16
    ./sample_point_pillars
    ```

3. Verify that the sample ran successfully. If the sample runs successfully you should see output similar to the following:
    ```
    Detected 11 objects
    Class Name, x, y, z, dx, dy, dz, yaw, score
    ===========================================
    car, 58.850220, 16.538342, -0.731842, 3.912604, 1.571266, 1.474371, 3.192682, 0.497436
    car, 29.562414, -7.192303, -0.701558, 3.425030, 1.459911, 1.462682, 6.118636, 0.385868
    cyclist, 46.127785, -4.656827, -0.186010, 1.711130, 0.447052, 1.760048, 6.462295, 0.342366
    car, 33.714344, 12.249531, -0.821620, 4.477329, 1.658763, 1.517633, 3.131651, 0.164527
    car, 46.800461, 23.382545, -1.060518, 3.884820, 1.620610, 1.490203, 3.029791, 0.158109
    car, 39.836563, 20.155823, -1.043305, 3.874372, 1.635187, 1.564985, 3.212456, 0.157651
    car, 61.725403, -3.649807, -0.079978, 4.120826, 1.612140, 1.644002, 3.003809, 0.153144
    car, 39.765160, 25.214563, -1.258425, 3.642243, 1.536955, 1.464124, 3.533899, 0.137190
    pedestrian, 44.991489, 24.765362, -0.685905, 0.627114, 0.659062, 1.918059, 4.205900, 0.130729
    car, 37.252636, -3.709802, -0.656006, 4.087790, 1.656895, 1.479053, 6.131449, 0.118882
    car, 55.934460, -3.352325, -0.268257, 4.440871, 1.718843, 1.612850, 3.341351, 0.101859
    ```
   This output shows that the sample ran successfully; `PASSED`.


### Sample `--help` options

To see the full list of available options and their descriptions, use the `-h` or `--help` command line option.


# Additional resources

The following resources provide a deeper understanding about samplePointPillars.

**Documentation**
- [Introduction To NVIDIA’s TensorRT Samples](https://docs.nvidia.com/deeplearning/sdk/tensorrt-sample-support-guide/index.html#samples)
- [Working With TensorRT Using The C++ API](https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#c_topics)
- [NVIDIA’s TensorRT Documentation Library](https://docs.nvidia.com/deeplearning/sdk/tensorrt-archived/index.html)

# License

For terms and conditions for use, reproduction, and distribution, see the [TensorRT Software License Agreement](https://docs.nvidia.com/deeplearning/sdk/tensorrt-sla/index.html) documentation.


# Changelog

Dec 2021
This is the first release of the `README.md` file and sample.


# Known issues

There are no known issues in this sample.
